---

layout: post

title: "Construindo um preditor"

date: 2020-11-21 23:50:41 

lang: R
category: Introdução
description: "Apresenta os passos básicos para a construção de um algoritmo preditor"
---


## *Design* de predição{#link16}

**1. Defina sua taxa de erro (*benchmark*).**

Por hora iremos utilizar uma taxa de erro genérica, mas em um próximo momento iremos falar sobre as diferentes taxas de erro possíveis que você pode escolher. 

Por exemplo, podemos calcular o chamado **erro majoritário**, que é o limite máximo abaixo do qual o erro de um **classificador** deve estar. Ele é dado por $1-p$, onde $p$ é a [taxa de não-informação](#link19) (que é a proporção da categoria mais frequente na variável de interesse). Por exemplo, se a variável de interesse possui 2 categorias, A e B, e se $85\%$ dos dados estão rotulados na categoria A e $15\%$ na categoria B, entao temos que a categoria A é a **classe majoritária** e $100\%-85\% = 15\%$ é o erro majoritário.

> Caso o erro do preditor seja superior ao erro majoritário, seria melhor classificar toda nova amostra na classe majoritária, certo? **Depende.**

Digamos que um psicólogo queira construir um classificador para prever se uma pessoa tem ou não indeação suicida, ou seja, pensa ou planeja suicídio. Suponha que ele tenha uma base de dados com 1000 observações, cuja variável de interesse seja "Tem indeação suicida?" e esteja rotulada com "sim" ou "não". Suponha, também, que 97% das observações, no caso indivíduos/pacientes, não possuem tal característica, e portanto 3% dos indivíduos possuem. Criado o preditor, observamos que o erro é de 5%, assim como mostrado a seguir:

![tela_0]({{ site.url }}/assets/r/courses/machine_learning/01/images/erroMajo.jpeg)

As partes em vermelho mostram o erro cometido por ambos os métodos. Agora note as pessoas que possuem indeação suicída, porém foram classificadas como não possuidoras dessa característica. O quanto isso afetará no diagnóstico do psicólogo?

**2. Definimos quais variáveis serão utilizadas para estimação dos parâmetros do classificador/regressor (função preditora).**

Nem sempre utilizar todas as variáveis do banco de dados é importante para o modelo. Pode acontecer de termos variáveis que não ajudam na predição, como por exemplo aquelas com uma variância quase zero (frequência muito alta de um único valor). Iremos estudar algumas formas de selecionar as melhores variáveis para o modelo em breve.

**3. Definimos o método que será utilizado para construção do classificador/regressor.**

Isso poderá ser feito, por exemplo, utilizando o método de validação cruzada (*cross-validation*), que será explicado detalhadamente em um capítulo mais à frente.

**4. Divida os dados em Treino e Teste, ou Treino, Teste e Validação (opcional).**

Como já comentado, o conjunto de treino deve ser criado para construir o seu modelo e o conjunto de testes para avaliar o seu modelo. Fazemos isso com o intuito de criarmos um modelo que se ajuste bem a qualquer base de dados, e não apenas à nossa. É comum usar 70% da amostra como treino e 30% como teste, mas isso não é uma regra. Podemos também repartir os dados em treino, teste e validação(*). É importante ficar claro que quem está conduzindo as análises é quem fica encarregado de decidir o que fica melhor para cada amostra.

**5. Utilizando a amostra TREINO, definimos os parâmetros da função preditora (classificador/regressor), obtendo o melhor modelo possível.**

**6. Aplicamos o melhor modelo obtido na amostra TESTE uma única vez, para estimar o erro do preditor.**

Aplicamos o melhor modelo na amostra teste apenas uma vez porque se aplicarmos diversas vezes até achar o melhor modelo estaremos utilizando o teste, de certa forma, para treinar o modelo, pois o ajuste do modelo seria influenciado pelo resultado do teste. Isso não é desejável pois o objetivo do teste é nos servir como uma "nova amostra".

> (\*) Opcionalmente poderá ser criado um conjunto de **validação**, com o intuito de servir como um "pré-teste", que também será usado para avaliar seu modelo. Quando repartimos o conjunto de dados dessa forma, utilizamos o treino para construir o modelo, avaliamos o modelo na validação (ou seja, o ajuste do modelo é influenciado por ela), e se o resultado não for bom, retornamos ao treino para ajustar um outro modelo. Então novamente testamos o modelo na validação, e assim sucessivamente até acharmos um modelo que se adequou bem tanto ao treino quanto à validação. Aí, finalmente, aplicamos ele ao conjunto teste, avaliando na prática a sua qualidade.
