---

layout: post

title: "Erros Amostrais"

date: 2020-11-21 23:50:41 

lang: R
category: Introdução
description: "Define os erros dentro e fora da amostra (in sample and out of sample error)"
---


## Erros Amostrais

Este é um dos conceitos mais fundamentais com os quais lidamos na aprendizagem de máquina e previsão. Temos duas taxas de erros amostrais: o **erro dentro da amostra (*in sample error*)** e o **erro fora da amostra (*out of sample error*)**.

### Erro dentro da Amostra (*In Sample Error*)

É a taxa de erro que você recebe no mesmo conjunto de dados usado para criar seu preditor. Na literatura às vezes é chamado de erro de resubstituição.

Em outras palavras, é quando seu algoritmo de previsão se ajusta ao que você coletou num conjunto de dados específico. E assim, quando você recebe um novo conjunto de dados, a precisão diminuirá.

### Erro fora da Amostra (*Out of Sample Error*)

É a taxa de erro que você recebe em um novo conjunto de dados. Na literatura às vezes é chamado de erro de generalização. 

Uma vez que coletamos uma amostra de dados e construímos um modelo para ela, podemos querer testá-lo em uma nova amostra, por exemplo uma amostra coletada em um horário diferente ou em um local diferente. Daí podemos analisar o quão bem o algoritmo executará a predição nesse novo conjunto de dados.

### Algumas ideias-chave {#link4}

1. O erro fora da amostra é o que mais interessa.
2. O erro dentro da amostra é quase sempre menor que o erro fora da amostra.
3. Um erro frequente é ajustar muito o algoritmo aos dados que temos. Em outras palavras, criar um modelo *overfitting*(*). 

> (\*) **Overfitting** é um termo usado na estatística para descrever quando um modelo estatístico se ajusta muito bem a um conjunto de dados anteriormente observado e, como consequência, se mostra ineficaz para prever novos resultados. Uma tradução livre para esse termo seria "sobreajustado".

Vejamos um exemplo de erro dentro da amostra *vs* erro fora da amostra:


```r
set.seed(131)

# Vamos selecionar as linhas da base de dados spam através de uma amostra de tamanho 10 das 4601 linhas
# dos dados:
spamMenor = spam[sample(dim(spam)[1], size = 10), ]

# Vamos criar um vetor composto pelos rótulos "1" e "2". 
# Se um e-mail da nossa amostra for spam, recebe "1", se não for spam, recebe "2".
spamRotulos = (spamMenor$type == "spam")*1 + 1

# Na nossa base a variável capitalAve representa a média de letras maiúsculas por linha.
plot(spamMenor$capitalAve, col = spamRotulos, xlab = "Quantidade de Letras Maiúsculas",
     ylab = "Frequência", main = "Letras Maiúsculas em spam (vermelho) e em ham (preto)",
     pch = 19)
```

![tela_0]({{ site.url }}/assets/r/courses/machine_learning/01/images/unnamed-chunk-8-1.png)

Podemos notar que, em geral, as mensagens classificadas como spam possuem uma frequência maior de letras maiúsculas do que as mensagens classificadas como não spam. Com base nisso queremos construir um preditor, onde podemos classificar e-mails como spam se a frequência de letras maiúsculas for maior que uma determida constante, e não spam caso contrário.

Veja que se separarmos os dados pela frequência de letras maiúsculas maior que 2,5 e classificarmos o que está acima como spam e abaixo como não spam, ainda teríamos duas observações que não são spam acima da linha.


```r
plot(spamMenor$capitalAve, col = spamRotulos, xlab = "Quantidade de Letras Maiúsculas",
     ylab = "Frequência", main = "Letras Maiúsculas em spam (vermelho) e em ham (preto)",
     pch = 19)
abline(h = 2.5, lty = 3, col = "blue")
```

![tela_0]({{ site.url }}/assets/r/courses/machine_learning/01/images/unnamed-chunk-9-1.png)

Então o melhor para esse caso é criar o seguinte modelo:

- letras maiúsculas > 2,5 e < 3,8 $\Rightarrow$ spam;
- letras maiúsculas < 2,5 ou > 3,8 $\Rightarrow$ não spam.


```r
plot(spamMenor$capitalAve, col = spamRotulos, xlab = "Quantidade de Letras Maiúsculas",
     ylab = "Frequência", main = "Letras Maiúsculas em spam (vermelho) e em ham (preto)",
     pch = 19)
abline(h = c(2.5, 3.8), lty = 3, col = "blue")
```

![tela_0]({{ site.url }}/assets/r/courses/machine_learning/01/images/unnamed-chunk-10-1.png)


```r
# construindo o modelo sobreajustado
modelo.sobreajustado = function(x){
  predicao = rep(NA, length(x))
  predicao[(x>=2.5 & x<=3.8)] = "spam"
  predicao[(x<2.5 | x>3.8)] = "nonspam"
  return(predicao)
}
# avaliando o modelo sobreajustado
resultado = modelo.sobreajustado(spamMenor$capitalAve)
table(resultado, spamMenor$type)
```

```
##          
## resultado nonspam spam
##   nonspam       7    0
##   spam          0    3
```

Note que obtivemos uma precisão perfeita nessa amostra, como já era esperado. Nesse caso, o erro dentro da amostra é de 0%. Mas será que esse modelo é o mais eficiente em outros dados também?

Vamos usar essa segunda regra para criarmos um modelo mais geral:

- letras maiúsculas > 2,5 $\Rightarrow$ spam;
- letras maiúsculas <= 2,5 $\Rightarrow$ não spam.


```r
plot(spamMenor$capitalAve, col = spamRotulos, xlab = "Quantidade de Letras Maiúsculas",
     ylab = "Frequência", main = "Letras Maiúsculas em spam (vermelho) e em ham (preto)",
     pch = 19)
abline(h = 2.5, lty = 3, col = "blue")
```

![tela_0]({{ site.url }}/assets/r/courses/machine_learning/01/images/unnamed-chunk-12-1.png)

```r
# construindo o modelo geral
modelo.geral = function(x){
  predicao = rep(NA, length(x))
  predicao[x>=2.5] = "spam"
  predicao[x<2.5] = "nonspam"
  return(predicao)
}
# avaliando o modelo geral
resultado2 = modelo.geral(spamMenor$capitalAve)
table(resultado2, spamMenor$type)
```

```
##           
## resultado2 nonspam spam
##    nonspam       5    0
##    spam          2    3
```

Observe que dessa forma temos um erro dentro da amostra de 20%. Vamos agora aplicar esses dois modelos para toda a base de dados:


```r
table(modelo.sobreajustado(spam$capitalAve), spam$type)
```

```
##          
##           nonspam spam
##   nonspam    2297 1385
##   spam        491  428
```

```r
table(modelo.geral(spam$capitalAve), spam$type)
```

```
##          
##           nonspam spam
##   nonspam    2042  540
##   spam        746 1273
```

Olhando para a precisão de nossos modelos:


```r
sum(modelo.sobreajustado(spam$capitalAve) == spam$type)
```

```
## [1] 2725
```

```r
sum(modelo.geral(spam$capitalAve) == spam$type)
```

```
## [1] 3315
```

Observe que utilizando o modelo sobreajustado obtivemos um erro fora da amostra de 40,77%, enquanto que com o modelo geral esse erro foi de 27,95%. Note que se queremos construir um modelo que melhor representa qualquer amostra que pegarmos, um modelo **não sobreajustado** possuirá uma precisão maior.